# 2024-11-10-TIL

안녕하세요.

오늘은 11월 2주차에 공부한 내용에 대해 복습해보도록 하겠습니다.

# CPU 성능 향상을 위한 설계

## CPU 클럭 속도

- CPU를 구매하려고 하면 판매 페이지마다 빠지지 않고 등장하는 키워드 => **클럭(clock)**

### 클럭(clock)

- **컴퓨터의 부품을 일사불란하게 움직일 수 있게 하는 시간의 단위**
- 클럭의 '똑-딱-똑-딱' 주기에 맞춰 이 레지스터에서 다른 레지스터로 데이터가 이동하거나 ALU에서 연산이 수행되고, 메모리에 저장된 명령어를 읽어 들이는 것.
- 클럭 속도
  - 헤르츠(Hz) 단위로 측정
  - 이는 클럭이 1초에 몇 번 반복되는 지를 나타냄.
  - 예를 들어, 클럭이 '똑-딱'하고 1초에 100번 반복되는 CPU가 있다면, 이 CPU의 클럭 속도는 100Hz인 셈
  - 최근에는 CPU의 클럭 속도가 매우 빨라져서 더 빠른 등급인 기가헤르츠(GHz) 단위로 측정하는 것이 일반적
  - 클럭 속도가 높아지면 CPU는 명령어 사이클을 더 빠르게 반복하고, 다른 부품들도 그에 발맞춰 더 빠르게 작동할 것이라고 기대할 수 있음
  - 클럭 속도가 높은 CPU가 일반적으로 성능이 좋음
  - 이런 점에서 클럭 속도는 CPU의 속도 단위로 간주되기도 함. 하지만, 클럭 속도를 필요 이상으로 높이면 컴퓨터의 발열이 심해질 수 있기 때문에 클럭 속도를 높이는 것만으로 CPU의 성능을 높이는 데에는 한계가 있음

### 멀티코어와 멀티 스레드

- 클럭 속도를 높이는 방법 외에도 코어 수나 스레드 수를 늘리는 방법으로 CPU의 성능을 높일 수 있음.

#### 코어(Core)

- CPU 내에서 명령어를 읽어 들이고, 해석하고, 실행하는 부품을 의미
- 전공 서적들의 전통적 관점에서 '명령어를 읽어 들이고, 해석하고, 실행하는 부품'은 원칙적으로 CPU 하나만 존재할 수 있었기 때문. 하지만, 오늘날 기술적 발전을 거듭한 CPU 안에는 '명령어를 읽어 들이고, 해석하고, 실행하는 부품'들 여러 개가 얼마든지 존재할 수 있게 되었고, 이에 코어라는 이름을 붙인 것
- 어떤 CPU를 설명하는 글에 코어가 8개라고 명시되어 있다면, CPU 내부에서 명령어를 읽어들이고, 해석하고, 실행하는 부품이 8개라는 뜻.
- 이렇게 여러 코어를 포함하고 있는 CPU는 **멀티코어 CPU**, 혹은 **멀티코어 프로세서**라고 부름

#### 스레드

- 사전적 의미
  - 실행 흐름의 단위
- 혼란을 방지하기 위해 CPU에서 사용하는 **하드웨어적인 스레드(이하 하드웨어 스레드)**와 프로그래밍 언어 및 운영체제에서 사용하는 **소프트웨어적인 스레드(이하 스레드)**로 나누어 기억하길!
- 하드웨어 스레드
  - **하나의 코어가 동시에 처리하는 명령어의 단위** 
  - 하나의 코어로 여러 명령어를 동시에 처리하는 CPU를 '멀티 스레드 프로세서' 혹은 멀티스레드 CPU라고 함
  - 예를 들어, 명령어를 읽어 들이고, 해석하고, 실행하는 부품 하나가 한 번에 하나의 명령어를 처리한다면 이는 1코어 1스레드 CPU이고, 명령어를 읽어 들이고, 해석하고, 실행하는 부품 2개가 한번에 4개의 명령어를 처리한다면 이는 2코어 4스레드 CPU인 것임.
  - 메모리 속 프로그램의 입장에서 봤을 때 '한 번에 하나의 명령어를 처리하는 1개의 CPU'와 다를 바 없음.
  - 마찬가지로 2코어 4스레드 CPU를 메모리 속 프로그램의 입장에서 보면 '한 번에 하나의 명령어를 처리하는 4개의 CPU'가 있는 것처럼 보일 것임. 그래서 하드웨어 스레드를 **논리 프로세서(logical processor)**라고 부르기도 함.
- 소프트웨어 스레드
  - **하나의 프로그램에서 독립적으로 실행되는 단위**
  - '어떤 프로그램이 여러 (소프트웨어적) 스레드를 통해 실행될 수 있다'는 말은 '메모리에 적재된 해당 프로그램을 구성하는 여러 부분이 동시에 실행될 수 있다'라는 말과 같음

### 병렬성과 동시성 

#### 병렬성

- 작업을 물리적으로 동시에 처리하는 성질.
- 하드웨어 스레드가 4개인 CPU가 4개의 명령어를 동시에 실행하는 경우를 생각해보자.
- 같은 시점에 여러 작업을 동시에 처리할 수 있을 것!

#### 동시성

- 동시에 작업을 처리하는 것처럼 보이는 성질을 의미
- CPU가 빠르게 작업을 번갈아 가며 처리할 경우, 사용자의 눈에는 마치 여러 작업이 동시에 처리되는 것처럼 보일 수 있지만, 물리적으로 같은 시점에 여러 작업이 동시에 처리되고 있는 것은 아님.

하드웨어 스레드는 '병렬성'을 구현하기 위한 물리적인 실행 단위에 가깝고, 소프트웨어 스레드는 '동시성'을 구현하기 위한 논리적인 실행 단위에 가깝다.

# 파이프라이닝을 통한 명령어 병렬 처리

- **명령어 병렬 처리 기법(Instruction-Level Parallelism)**은 여러 명령어를 동시에 처리하여 CPU를 한시도 쉬지 않고 작동시킴으로써 CPU의 성능을 높이는 기법을 말함.

- 현대 CPU의 명령어 처리에서 절대 빠질 수 없는 핵심 기술이라고 할 수 있음.

- 명령어 파이프라이닝을 이해하려면 우선 하나의 명령어가 처리되는 과정을 비슷한 시간 간격으로 나누어 보야야 함. 일반적으로 다음과 같은 간격으로 나눌 수 있음

  1. 명령어 인출(Instruction Fetch)
  2. 명령어 해석(Instruction Decode)
  3. 명령어 실행(Execute Instruction)
  4. 결과 저장(Write Back)

- 여기서 중요한 점은 같은 단계가 겹치지만 않는다면 CPU가 각각의 단계를 동시에 실행할 수 있다. CPU는 하나의 명령어가 인출하는 동안 다른 명령어를 실행할 수 있고, 하나의 명령어가 실행되는 동안 연산의 결과를 저장할 수 있음.

- 공장의 생산 라인과 같이 명령어들을 **명령어 파이프라인(Instruction pipeline)**에 넣고, 동시에 처리하는 기법을 **명령어 파이프라이닝(Instruction pipelining)**이라고 함.

- 파이프라이닝 성능의 차이를 보이는 대표적인 명령어 집합 유형

  - **CISC(Complex Instruction Set Computer)**
    - CISC 기반의 CPU => Intel x86, x86-64 CPU
    - 이름 그대로 다채로운 기능을 지원하는 복잡한 명령어들로 구성된 명령어 집합
    - 적은 수의 명령어로도 프로그램을 실행할 수 있음
    - 다만, 활용하는 명령어가 워낙 복잡하고 다양한 기능을 제공하는 탓에 명령어의 크기 및 실행되기까지의 시간이 일정하지 않고, 하나의 명령어 실행에 여러 클럭 주기가 필요
    - RISC에 비해 명령어 수행 시간이 길고, 들쑥날쑥하기 때문에 파이프라이닝에 비효율적일 수 있음
  - **RISC(Reduced Instruction Set Computer)**
    - RISC 기반의 CPU => Apple의 M1 CPU
    - CISC에 비해 활용 가능한 명령어의 종류가 작음.
    - CISC와는 달리 짧고, 규격화된 명령어, 되도록이면 1클럭 내외로 실행되는 명령어를 지향
    - 같은 프로그램이라 하더라도 RISC에는 CISC보다 많은 명령어가 필요
    - 파이프라이닝에 최적화

- 파이프라이닝이 CPU 성능 향상에 실패하는 경우

  - 파이프라이닝이 실패하여 성능 향상이 이루어지지 않는 상황은 **파이프라인 위험**이라고 부르며, **데이터 위험**과 **제어 위험**, **구조적 위험**으로 구분할 수 있음.
    - 데이터 위험
      - 명령어 간의 데이터 의존성에 의해 발생
      - 어떤 명령어들은 동시에 처리할 수 없고, 이전 명령어를 끝까지 실행해야만 비로소 실행할 수 있음
      - 예를 들어, 다음의 두 명령어 중 명령어 2는 명령어 1에 의존적
      - 명령어 1의 저장까지 끝낸 뒤에야 비로소 명령어 2를 인출할 수 있기 때문
    - 제어 위험
      - 프로그램 카운터의 갑작스러운 변화에 의해 발생합니다.
      - 프로그램 카운터는 기본적으로 1씩 증가하며, '현재 실행 중인 명령어의 다음 주소'로 갱신
      - 하지만, JUMP, CONDITIONAL JUMP, 인터럽트 등으로 인해 프로그램 실행의 흐름이 바뀌어 명령어가 실행되면서 프로그램 카운터 값에 갑작스러운 변화가 생기면 미리 인출하거나 해석 중인 명령어들은 아무 쓸모가 없어지게 됨.
    - 구조적 위험
      - 명령어를 겹쳐 실행하는 과정에서 서로 다른 명령어가 동시에 ALU, 레지스터 등 같은 CPU 부품을 사용하려고 할 때 발생.
      - 자원 위험이라고도 부름

  # RAM

  - 전원을 끄면 저장하고 있던 데이터와 명령어가 날아가는 휘발성 저장장치이며, CPU가 실행할 대상을 저장하는 부품
  - CPU는 보조기억장치에 저장된 프로그램을 곧장 가져와 실행할 수 없기 때문에 어떠한 프로그램을 실행하고자 한다면 프로그램을 보관하고 있는 보조기억장치에서 메모리로 복사해 와야 함.
  - RAM의 용량은 컴퓨터에 큰 영향을 끼침. RAM의 용량이 작으면 보조기억장치로부터 실행할 프로그램을 가지고 오는 일이 잦아져 실행할 프로그램을 가지고 오는 일이 잦아져 실행 시간이 길어지지만, RAM의 용량이 충분히 크면 보조기억장치로부터 많은 데이터를 가져와 미리 RAM에 저장할 수 있기 때문에 많은 프로그램을 동시에 실행하는 데 유리.
  - RAM(Random Access Memory), Random Access(임의 접근)이란 저장된 요소에 순차적으로 접근할 필요 없이 임의의 위치에 곧장 접근 가능한 방식을 의미. 그래서 직접 접근이라고도 부름
  - 예를 들어, 100번지에 있는 데이터에 접근하고자 할 때, 1번지, 2번지 ... 100번지 순으로 접근할 필요 없이 곧장 100번지로 접근하는 것이 가능하다는 것.
  - 임의 접근과 반대 되는 개념으로는 순차 접근이 있음. 이름 그대로 특정 위치에 저장된 요소에 접근하기 위해 처음부터 순차적으로 접근하는 방식

  ## RAM의 종류

  ### DRAM(Dynamic RAM)

  - Dynamic (동적인)은 말 그대로 저장된 데이터가 동적으로 변하는 (사라지는) 특성을 의미.
  - 즉, DRAM은 **시간이 지나면 저장된 데이터가 점차 사라지는 RAM**임.
  - 데이터 소멸을 막기 위해 일정 주기로 데이터를 재활성화 (다시 저장) 해야 함.
  - 이런 단점에도 불구하고 DRAM을 메모리로 사용하는 것이 일반적. 비교적 DRAM의 소비 전력이 낮고, 저렴하며, 집적도가 높아 메모리를 대용량으로 설계하기 용이하기 때문

  #### SRAM(Static RAM)

  - DRAM과는 달리, 저장된 데이터가 변하지 않는 RAM을 의미
  - 즉, 시간이 지나도 저장된 데이터가 사라지지 않는 RAM(저장된 데이터가 사라지지 않는다고 해서 SRAM이 비휘발성 저장장치라는 것은 아님)(SRAM도 전원이 공급되지 않으면 자장된 내용이 소실됨)
  - 일반적으로 SRAM은 DRAM과 비교해 속도는 빠르지만, 소비 전력이 크고 가격도 비싼데다 집적도도 낮기 때문에 대용량으로 만들 필요는 없지만, 속도가 빨라야 하는 저장장치, 가령 **캐시 메모리** 등에 사용

  ### SDRAM(Synchronous Dynamic RAM)

  - **클럭 신호와 동기화**된, 보다 발전된 형태의 DRAM을 말함. (SRAM과 DRAM의 합성어가 아니라는 점에 주의하기 바람)
  - 클럭 신호와 동기화되었다는 것은 클럭 타이밍에 맞춰 CPU와 정보를 주고받을 수 있다는 것을 의미
  - 다시 말해, SDRAM은 클럭에 맞춰 작동하며 CPU와 정보를 주고받을 수 있는 DRAM을 말함

  ### DDR SDRAM(Double Data Rate SDRAM)

  - 대역폭을 넓혀 속도를 빠르게 만든 SDRAM을 말함.
  - 대역폭 => 데이터를 주고받을 길의 너비
  - SDRAM이 한 클럭당 한 번씩 CPU와 데이터를 주고받을 수 있다면, DDR SDRAM은 두 배의 대역폭으로 한 클럭당 두 번씩 CPU와 데이터를 주고받을 수 있음. 말하자면, DDR SDRAM은 **너비가 두 배인 자동차 도로와** 같음.
  - 따라서, DDR SDRAM은 한 클럭당 하나씩 데이터를 주고 받을 수 있는 SDRAM보다 **전송 속도가 두 배 가량 빠름**

# 메모리에 바이트를 밀어 넣는 순서 - 빅 엔디안과 리틀 엔디안

- 현대의 메모리는 대부분 데이터를 **바이트** 단위로 저장하고 관리
- 하지만, 메모리는 데이터를 CPU로부터 바이트 단위로 받아들이지 않고, 일반적으로 4바이트(32비트), 혹은 8바이트(64비트)인 워드 단위로 받아들임.
- 여러 바이트로 구성된 데이터를 받아들여 여러 주소에 걸쳐 저장하게 됨.
- 다시 말해, 한 주소에 1바이트씩을 저장하는 메모리는 4바이트의 데이터를 4개의 주소에 저장하고, 8바이트의 데이터를 8개의 주소에 저장함.
- 메모리에 바이트를 저장하는 방식은 연속해서 저장해야 하는 바이트를 어떤 순서로 저장하는지에 따라 빅 엔디안과 리틀 엔디안으로 나눌 수 있음.

## 빅 엔디안(big endian)

- 낮은 번지의 주소에 **상위 바이트로부터 저장하는 방식**을 말함.
- 여기서 상위 바이트는 **가장 큰 값**이라고 생각해도 무방함
- 예를 들어, 10진수 123에서 가장 큰 수는 당연히 100을 나타내는 1일 것임.
- 마찬가지로 16진수 1A2B3C4D에서 가장 큰 수, 최상위 바이트(최상위 8비트)는 1A임
- 우리가 일상적으로 숫자 체계를 읽고 쓰는 순서와 동일하기 때문에 메모리 값을 직접 읽거나, 특히 디버깅할 때 편리

## 리틀 엔디안(little endian)

- 낮은 번지의 주소에 **하위 바이트로부터 저장하는 방식**
- 하위 바이트는 상위 바이트와 반대로, **가장 작은 값**을 의미
- 10진수 123에서 가장 작은 값이 3이듯, 16진수인 1A2B3C4D인 최하위 바이트는 4D임.
- 메모리 값을 직접 읽고 쓰기는 불편하지만 수치 계산이 편리하다는 장점이 있음.
- 예를 들어, 우리가 123 + 456이라는 덧셈을 수행할 때, 가장 작은 값인 일의 자릿수 3과 6부터 계산해 나가듯 가장 작은 값부터 저장되어 있는 데이터 시작점에서 수치를 계산해 나가거나 자리올림할 수 있음

# 캐시 메모리

- CPU는 프로그램 실행 과정에서 빈번하게 메모리에 접근해야만 합니다.
- 하지만, CPU가 메모리에 접근하는 속도는 CPU가 레지스터에 접근하는 속도보다 느리기 때문에 CPU의 연산 속도가 아무리 빨라도 메모리에 접근하는 속도가 느리면 CPU의 빠른 연산 속도는 아무 효용이 없음. 
- 그래서 등장한 저장장치가 바로 캐시 메모리임.
- 캐시 메모리는 CPU의 연산 속도와 메모리 접근 속도의 차이를 줄이기 위해 탄생한 저장장치로, CPU와 메모리 사이에 위치한 SRAM 기반의 저장장치임.
- CPU가 매번 메모리에 왔다 갔다하는 시간이 오래걸리므로 메모리에서 CPU가 사용할 일부 데이터를 미리 캐시 메모리로 가져와 활용하자는 것임.
- 컴퓨터 내부에는 여러 종류의 캐시 메모리가 존재
- 이중 코어와 가장 가까운 캐시 메모리를 L1 캐시(Level 1 cache), 그 다음으로 가까운 캐시 메모리를 L2(Level 2 cache), 그 다음으로 가까운 캐시 메모리를 L3 캐시(Level 3 cache)라고 부름.
- 일반적으로 L1 캐시와 L2캐시는 코어 내부에, L3캐시는 코어 외부에 위치
- 캐시 메모리의 크기
  - L1 < L2 < L3
- 캐시 메모리의 속도
  - L3 < L2 < L1 의 순으로 빠름
- CPU가 메모리 내에 데이터가 필요하다고 판단하면 우선 L1 캐시 메모리에 해당 데이터가 있는지 알아보고, 없다면 L2, L3 캐시 메모리 순으로 데이터를 검색
- 멀티코어 프로세서의 경우 일반적으로 L1 캐시 메모리와 L2 캐시 메모리는 코어마다 고유한 캐시 메모리로 할당되고, L3 캐시는 여러 코어가 공유하는 형태로 구현됨
- 또한, 코어와 가장 가까운 L1 캐시 메모리는 명령어만을 저장하는 **L1l**캐시와 데이터만을 저장하는 L1캐시인 **L1D**캐시로 구분하기도 하며, 이러한 유형의 캐시 메모리를 **분리형 캐시(split cache)**라고 함.

### 캐시 히트와 캐시 미스

- 캐시 메모리는 메모리보다 용량이 작기 때문에 메모리에 있는 모든 내용을 캐시 메모리에 가져와 저장할 수는 없음.

- 메모리가 보조기억장치의 일부를 복사하여 저장하는 것처럼 캐시 메모리도 메모리의 일부를 복사하여 저장함

- 캐시 메모리에는 무엇을 저장할까?

  - 보조저장장치가 (전원이 꺼져도) 보관할 것을 저장하고, 메모리가 실행 중인 것을 저장한다면 캐시 메모리는 **CPU가 사용할 법한** 것을 저장

  - 이렇게 캐시 메모리가 예측하여 저장한 데이터가 CPU에 의해 실제로 사용되는 경우를 **캐시 히트(cache hit)**라고 하며, 반대로 자주 사용될 것으로 예측하여 캐시 메모리를 저장했지만 틀린 예측으로 인해 CPU가 메모리로부터 필요한 데이터를 직접 가져와야 하는 경우를 **캐시 미스(cache miss)**라고 함.
  - 만약, 캐시 미스가 발생한다면 캐시 메모리의 이점을 활용할 수 없게 되고, 캐시 히트에 비해 CPU의 성능이 떨어짐.
  - 참고로, 캐시가 히트되는 비율을 캐시 적중률(cache hit ratio)이라고 하며, 다음과 같이 계산함. 범용적으로 사용되는 컴퓨터의 캐시 적중률은 대략 85~95% 이상임.
    - 캐시 히트 횟수 / (캐시 히트 횟수 + 캐시 미스 횟수)

### 참조 지역성의 원리(locality of reference, principle of locality)

- 캐시 메모리의 이점을 제대로 활용하려면 CPU가 사용할 법한 데이터를 제대로 예측해서 캐시 적중률을 높여야 함.
- 그렇다면 CPU가 사용할 법한 데이터는 어떻게 예측할 수 있을까?
- 캐시 메모리는 참조 지역성의 원리라는 특정한 원칙에 따라 메모리로부터 가져올 데이터를 결정함.
- 참조 지역성의 원리란, CPU가 메모리에 접근할 때 보이는 다음과 같은 주된 경향을 의미
  - 시간 지역성: CPU는 최근에 접근했던 메모리 공간에 다시 접근하려는 경향이 있음
  - 공간 지역성: CPU는 접근한 메모리 공간의 근처에 접근하려는 경향이 있음
- 시간 지역성을 가장 잘 보이는 사례는 프로그래밍 언어의 **변수**임. 일반적으로 변수에 저장된 값은 한 번만 사용되지 않고, 프로그램이 실행되는 동안 여러 번 사용됨. 즉, CPU는 최근에 접근했던 (변수가 저장된) 메모리 공간에 여러번 다시 접근할 수 있음. 이렇게 최근에 접근했던 메모리 공간에 다시 접근하려는 경향을 **시간 지역성**이라고 함.
- 접근한 메모리 공간의 근처에 접근하려는 경향인 공간 지역성을 단적으로 보여주는 사례는 **배열**임.

### 캐시 메모리의 쓰기 정책과 일관성

- CPU가 캐시 메모리에 데이터를 쓸 때는 캐시 메모리에 새롭게 쓰여진 데이터와 메모리 상의 데이터가 일관성을 유지해야 함
- 예를 들어, 현재 메모리에 1000번지에 200이라는 값이 저장되어 있고, 이 값이 캐시 메모리에도 저장되어 있다고 가정해 보자. CPU가 이 값에 접근하고자 할 때는 당연히 앞서 배웠던 것처럼 캐시 메모리를 통해 값을 얻어낼 것임.
- 이 때, 만약 CPU가 이 값을 200에서 300으로 바꾸고 싶다면 어떻게 해야할까? 곧장 메모리 1000번지에 달려가 값을 300으로 바꾸는 것은 좋은 생각이 아님. 현재 CPU는 1000번지 값을 얻기 위해 캐시 메모리를 참조하고 있기 때문에 메모리 1000번지 값을 무작정 300으로 바꾼다면 다음과 같은 명령어를 수행할 때 예상치 못한 결과가 나타날 수 있기 때문
  - 명령어1: 1000번지 값을 300으로 바꾸기 // 메모리 1000번지의 값: 300, 캐시 메모리 내의 값: 200
  - 명령어2: 1000번지 값 출력하기 // 캐시 메모리를 읽어 들일 경우 여전히 200을 출력
- 위와 같은 문제를 방지하기 위한 방법에는 크게 2가지가 있음
  - 하나는 캐시 메모리와 메모리에 동시에 쓰는 방법이 있을 수 있음. 이를 **즉시 쓰기(write-through)**라고 함. 즉시 쓰기는 메모리를 항상 최신 상태로 유지하여 캐시 메모리와 메모리 간의 일관성이 꺠지는 상황을 방지할 수 있지만, 데이터를 쓸 때마다 메모리를 참조해야 하므로 버스의 사용 시간과 쓰기 시간이 늘어난다는 단점이 있음
    - 메모리 접근을 최소화하기 위해 캐시 메모리를 만들었는데, 데이터를 쓸 때마다 메모리와 캐시 메모리에 동시에 접근해야 한다면 캐시 메모리를 둔 효율이 떨어질 것임.
  - 두번째는 캐시 메모리에만 값을 써두었다가 추후 수정된 데이터를 한 번에 메모리에 반영하는 방법이 있음. 이를 **지연 쓰기(write-back)**라고 함. 메모리 접근 횟수를 줄일 수 있어 즉시 쓰기 방식에 비해 속도는 더 빠르지만, 메모리와 캐시 메모리 간의 일관성이 깨질 수 있다는 위험을 감수해야 함.
- 캐시 메모리와 메모리 간의 불일치만 해결해야 하는 것이 아님. 때로는 다른 코어가 사용하는 캐시 메모리와의 불일치도 발생할 수 있음. 자칫 각기 다른 코어가 서로 다른 데이터를 대상으로 작업할 수 있기 때문
- 중요한 것은, 캐시 메모리를 사용한다는 것, 나아가 캐싱을 한다는 것은 데이터 접근에 있어 어느 정도의 빠른 성능은 보장할 수 있지만, 그와 동시에 데이터의 일관성을 유지하기 위한 책임이 따르는 방식이라는 것임.

<br/>

<br/>

이번 주에 공부한 내용은 여기까지 입니다.

읽어주셔서 감사합니다.