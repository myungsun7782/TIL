# 2024-10-27-TIL

안녕하세요.

오늘은 컴퓨터 구조에서 컴퓨터가 이해하는 정보에 대해 알아보도록 하겠습니다.

# 컴퓨터가 이해하는 정보

- CPU는 기본적으로 0과 1만을 이해할 수 있음
- 여기서 0과 1을 나타내는 가장 작은 정보의 단위를 비트(bit)라고 함.
- 1비트는 0 또는 1, 2개(2^1)의 정보를 표현할 수 있고, 2비트는 4개(2^2)의 정보, 3비트는 8개(2^3)의 정보를 표현할 수 있음.
- 즉, **N비트는 2^N개의 정보**를 표현할 수 있음을 알 수 있음.

| 구분   | 비트             |
| ------ | ---------------- |
| 1 byte | 8비트            |
| 1 kB   | 1,000바이트      |
| 1 MB   | 1,000 킬로바이트 |
| 1 GB   | 1,000 메가바이트 |
| 1 TB   | 1,000 기가바이트 |

- 바이트(byte)는 여덟 비트를 묶은 단위를 말하므로 하나의 바이트로 표현할 수 있는 정보는 2^8=256개
- 킬로바이트, 메가바이트, 기가바이트, 테라바이트 단위는 모두 이전 단위 1,000개를 묶은 단위를 말함
- 워드(word)
  - **CPU가 한번에 처리할 수 있는 데이터의 크기를 의미**
  - 만약, CPU가 한 번에 16비트를 처리할 수 있다면 1워드는 16비트가 되고, 한 번에 32비트를 처리할 수 있다면 32비트가 되는 것임.
  - 워드의 크기는 CPU마다 다르지만, 현대 컴퓨터 대부분의 워드 크기는 32비트 혹은 64비트임.

<br/>

## 데이터 - 0과 1로 숫자 표현하기

- 0과 1만을 이해할 수 있는 CPU는 컴퓨터 내부에서 2진법(binary)을 사용해 2 이상, 0 이하의 수를 이해함.
- 우리가 일상적으로 사용하는 10진법이 숫자 9를 넘어가는 시점에 자리올림해 0부터 9까지 10가의 숫자만으로 표현하듯, 컴퓨터가 사용하는 2진법 또한 숫자 1을 넘어가는 시점에 자리올림해 0과 1, 2개의 숫자만으로 모든 수를 표현함.
- 2진수로 표현된 수는 숫자 뒤에 아래첨자로 (2)를 붙이거나 2진수 앞에 0b를 붙임

### 16진법

- 표현하는 숫자의 길이가 너무 길어진다는 단점을 가지고 있는 2진법
- 16진수를 나타내는 16진법(hexadecimal)은 숫자 15를 넘어가는 시점에 자리올림을 하는 숫자표현 방식
- 16진수로 표현된 수는 뒤에 아래첨자로 (16)을 붙이거나 16진수 앞에 0x를 붙입니다. 16진수도 2진수 못지않게 많이 활용(MAC 주소, IPv6 주소를 표현할 때도 사용)

### 부동 소수점(floating point)

- 컴퓨터 내부에서 소수점을 나타내기 위해 대표적으로 사용하는 표현 방식
- 소수점이 고정되어 있지 않은 소수 표현 방식으로, 필요에 따라 소수점의 위치가 이동할 수 있고 유동적(floating)이라는 의미
- 예제
  - 10진수 123.123이라는 수를 m x 10^n의 꼴로 나타내면 1.23123 x 10^2으로 표현할 수 도 있고,1231.23 x 10^-1으로 표현할 수도 있음. 여기서 제곱으로 표현된 2와 -1을 **지수(exponent)**, 기울임체로 표기한  1.23123, 1231.23을 **가수(significand)**라고 함

#### 왜 아래 소스 코드의 결과는 0.30000000000000004일까?

```swift
var a = 0.1
var b = 0.2

print(a + b)
```

컴퓨터는 내부적으로 부동 소수점 방식을 통해 소수를 표현합니다. 부동 소수점 방식은 2진수로 소수를 표현하는 방식으로, 가수 x 2^지수 의 형태를 띕니다. 소스 코드에 제시된 0.1과 0.2는 10진수 소수로, 가수 x 10^지수 형태로 표현할 수 있지만, 이를 부동 소수점 방식의 2진수로 표현하면 무한 소수가 됩니다. 무한 소수에 무한 소수를 더한 결과를 10진수 소수로 표현하다 보니 오차가 발생한 것입니다.

<br/>

## 데이터 - 0과 1로 문자 표현하기

- 컴퓨터가 이해할 수 있는 문자들의 집합은 문자 집합(Character Set)이라고 함.
- 문자 집합에 속한 문자를 컴퓨터가 이해하는 0과 1로 이루어진 문자 코드로 변환하는 과정을 **문자 인코딩(character encoding)**이라고 함.
- 반대로, 0과 1로 표현된 문자를 사람이 이해하는 문자로 변환하는 과정을 **문자 디코딩(character decoding)**이라고 함

### 아스키(ASCII)

- 초창기 컴퓨터에서 사용하던 문자 집합 중 하나로, 영어의 알파벳과 아라비아 숫자, 일부 특수 문자를 포함
- 하나의 아스키 문자를 표현하기 위해서는 8비트(1바이트)를 사용
- 8비트 중 1비트는 **패리티 비트**라고 불리는데, 이는 **오류 검출을 위해 사용되는 비트**이기 때문에 **실질적으로 문자 표현을 위해 사용되는 비트는 7비트**임.
- 7비트로 표현할 수 있는 정보의 가짓수는 2^7개 이므로 총 128개의 문자를 표현할 수 있음

![image](https://github.com/user-attachments/assets/42cee4b9-8140-4638-88af-443d7de369af)

- 예를 들어, 'A'는 10진수 65(2진수 1000001(2))로 인코딩되고, 'a'는 10진수 97(2진수 1100001(2))로 인코딩된다.
- 다만, 아스키 코드는 한글을 표기할 수 없음. 그래서 등장한 인코딩 방식 중 하나가 **EUC-KR**
- EUC-KR 인코딩 방식을 사용하면 2,350개 정도의 한글 단어를 표현할 수는 있지만, 모든 한글 조합을 표현할 수 있을 정도로 많은 양은 아님

### 유니코드(unicode)

- 한글을 포함해 EUC-KR에 비해 훨씬 많은 언어, 특수문자, 화살표, 이모티콘까지 코드로 표현할 수 있는 **통일된 문자 집합**
- 유니코드가 없었다면 각각의 언어마다 다른 문자 집합과 인코딩 방식을 이해해야 했겠지만, 유니코드가 대부분의 언어를 지원하기 때문에 국가별로 다른 문자 집합과 인코딩 방식을 준비할 필요가 없어졌음

<br/>

오늘 공부한 내용은 여기까지입니다.

읽어주셔서 감사합니다.